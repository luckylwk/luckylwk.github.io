<!DOCTYPE html>
<html>

    <head>

                
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        
        <title>Setting up Apache Spark and IPython</title>
        
        <meta name="viewport" content="width=device-width">
        <meta name="description" content="Some thoughts and writeups.">
        
        <link rel="stylesheet" href="//storage.googleapis.com/code.getmdl.io/1.0.4/material.indigo-pink.min.css">
        <script src="//storage.googleapis.com/code.getmdl.io/1.0.4/material.min.js"></script>
        <!-- Icon-font -->
        <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
        <!-- Fonts -->
        <link href='//fonts.googleapis.com/css?family=Merriweather:400,300,700' rel='stylesheet' type='text/css'>
        <link href='//fonts.googleapis.com/css?family=Hind:300,400,500,600' rel='stylesheet' type='text/css'>

        <link rel="canonical" href="http://luckylwk.github.io/2015/07/30/setting-up-apache-spark/">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

        <link rel="icon" type="image/png" href="/assets/github-favicon.png">

        
        <style>
            .lwk-main-container {
                background: #f9f9f9;
            }
            .lwk-main-container .mdl-layout__header,
            .lwk-main-container .mdl-layout__drawer-button {
                color: white;
            }
            .mdl-layout__header {
                background: rgba(12,16,61,1);
            }
        </style>

    </head>

    <body>

        <div class="lwk-main-container mdl-layout mdl-js-layout">

            <!-- <div id="particles-js" class="particles-container"></div> -->
            <!-- // <script src="../js/particles.min.js"></script> -->
            <!-- // <script src="../js/app.js"></script> -->

                                

            <header class="mdl-layout__header mdl-layout__header">
                <div class="mdl-layout__header-row">
                    <!-- Title -->
                    <span class="mdl-layout-title">luckylwk</span>
                    <!-- Add spacer, to align navigation to the right -->
                    <div class="mdl-layout-spacer"></div>
                    <!-- Navigation -->
                    <nav class="mdl-navigation">
                        <a class="mdl-navigation__link" href="/machine-learning/">Machine Learning</a>
                        <a class="mdl-navigation__link" href="/data-visualisation/">Data Visualisation</a>
                        <a class="mdl-navigation__link" href="/data-engineering/">Data Engineering</a>
                        <a class="mdl-navigation__link" href="/about/">About</a>
                    </nav>
                </div>
            </header>
            <div class="mdl-layout__drawer">
                <span class="mdl-layout-title">luckylwk</span>
                <nav class="mdl-navigation">
                    <a class="mdl-navigation__link" href="/machine-learning/">Machine Learning</a>
                    <a class="mdl-navigation__link" href="/data-visualisation/">Data Visualisation</a>
                    <a class="mdl-navigation__link" href="/data-engineering/">Data Engineering</a>
                    <a class="mdl-navigation__link" href="/about/">About</a>
                    <a class="mdl-navigation__link" href="https://www.twitter.com/luckylwk" target="_blank">Twitter</a>
                    <a class="mdl-navigation__link" href="https://uk.linkedin.com/pub/luuk-derksen/a/80/956" target="_blank">LinkedIn</a>
                    <a class="mdl-navigation__link" href="https://github.com/luckylwk" target="_blank">Github</a>
                </nav>
            </div>

            <main class="mdl-layout__content">

                        <div class="wrap">
            <div class="post">

                <header class="post-header">
                    <h1>Setting up Apache Spark and IPython</h1>
                    <p class="meta">Jul 30, 2015 â€¢ Luuk Derksen</p>
                    <a href="https://twitter.com/share" class="twitter-share-button" data-text="Check out Setting up Apache Spark and IPython" data-via="luckylwk">Tweet</a>
                    <script>
                        !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');
                    </script>
                </header>

                <article class="post-content">
                <h3>Installing Spark/pySpark</h3>

<p>To ensure that all the setup work can easily be replicated in a cloud-environment I have chosen to set everything up inside a Virtual Machine. For this we use <a href="https://www.vagrantup.com/">Vagrant</a> and <a href="https://www.virtualbox.org/">VirtualBox</a>. In this post I will not go into too much detail on how both of these work or how they can be configured. I recommend you read some of the <strong>Vagrant</strong> documentation to learn more about how to, for example, assign more RAM or shared folders with your host machine.</p>

<h3>Setting up a Virtual Machine using Vagrant</h3>

<p>Create a new folder on your machine that will be the home of your Vagrant file. Once the folder has been created cd into it and initialize a vagrant virtual machine. In this case I have chosen for a standard Ubuntu 14.04 distribution.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>mkdir vagrant-spark
<span class="gp">$ </span><span class="nb">cd </span>vagrant-spark/

<span class="gp">$ </span>vagrant init ubuntu/trusty64
</code></pre></div>
<p>The <code>Vagrantfile</code> has been created and can now be edited to change any configurations is you want to. We are going to change two small things in the <code>Vagrantfile</code> to ensure the rest of these notes will work. Open the Vagrantfile in a text editor, delete everything and paste in the following:</p>
<div class="highlight"><pre><code class="language-" data-lang=""># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure(2) do |config|

  config.vm.box = "ubuntu/trusty64"

  # To share a folder on your drive add the following:
  # config.vm.synced_folder "&lt;SYSTEM-PATH TO YOUR FOLDER&gt;", "/home/vagrant/shared"

  config.vm.provider "virtualbox" do |vb|
      # Customize the amount of memory on the VM:
      vb.memory = "2048"
  end

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine. In the example below,
  # accessing "localhost:8080" will access port 80 on the guest machine.
  # config.vm.network "forwarded_port", guest: 80, host: 8080

  # Port forwarding for the IPython port
  config.vm.network "forwarded_port", guest: 8001, host: 8001

  # Spark port forwarding
  config.vm.network "forwarded_port", guest: 8080, host: 8080
  config.vm.network "forwarded_port", guest: 4040, host: 4040

end

</code></pre></div>
<p>To now start the VM we can use the command</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>vagrant up --provider<span class="o">=</span>virtualbox
</code></pre></div>
<p>This will bring up the VM. To stop it when you want to you can use the <code>vagrant halt</code> command. For now, obviously, we will need to keep it up to make sure we can start setting it up and install Spark.</p>

<p>To do this we will need to work from within the VM itself, so now SSH in to the machine using</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>vagrant ssh
</code></pre></div>
<h3>Setting up (VM) Ubuntu</h3>

<p>Install some basic things for Ubuntu to make sure some Python libraries will work.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>sudo apt-get install git htop python-dev build-essential libatlas-base-dev gfortran libevent-dev libpng-dev libjpeg8-dev libfreetype6-dev
</code></pre></div>
<h3>Install Java</h3>

<p>Make sure to install Java on the machine to enable Spark.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>sudo apt-add-repository ppa:webupd8team/java
<span class="gp">$ </span>sudo apt-get update
<span class="gp">$ </span>sudo apt-get install oracle-java7-installer

<span class="gp">$ </span>java -version
</code></pre></div>
<h3>Install Scala (Optional)</h3>

<p>Some of Spark&#39;s options are not available yet to be accessed and used through Python (e.g., GraphX and some MLLib modules), so we&#39;ll install Scala as well so we can work with those features at a later stage.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Download the Scala tarball.</span>
<span class="gp">$ </span>wget http://www.scala-lang.org/files/archive/scala-2.11.6.tgz

<span class="c"># Create a new folder to place the scala-installation in.</span>
<span class="gp">$ </span>sudo mkdir /usr/local/src/scala

<span class="c"># Extract the contents into the newly created directory.</span>
<span class="gp">$ </span>sudo tar xvf scala-2.11.6.tgz -C /usr/local/src/scala/

<span class="c"># Remove the tarball</span>
<span class="gp">$ </span>rm scala-2.11.6.tgz
</code></pre></div>
<p>Open your bash-profile using <code>nano ~/.bash_profile</code> and add the following two lines:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Scala environment variable and path.</span>
<span class="nb">export </span><span class="nv">SCALA_HOME</span><span class="o">=</span>/usr/local/src/scala/scala-2.11.6
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$SCALA_HOME</span>/bin:<span class="nv">$PATH</span>
</code></pre></div>
<h3>Download and Install Apache Spark</h3>

<p>First download and extract the Spark tarball.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">cd</span> <span class="nv">$HOME</span>

<span class="c"># Download the pre-built Spark tarball.</span>
<span class="gp">$ </span>wget http://mirror.ox.ac.uk/sites/rsync.apache.org/spark/spark-1.4.0/spark-1.4.0-bin-hadoop2.6.tgz 

<span class="c"># Extract the contents to your HOME folder.</span>
<span class="gp">$ </span>tar -xvf spark-1.4.0-bin-hadoop2.6.tgz

<span class="c"># Remove the tarball.</span>
<span class="gp">$ </span>rm spark-1.4.0-bin-hadoop2.6.tgz
</code></pre></div>
<p>Since this is a pre-built version we should be able to run it out-of-the-box now.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">cd </span>spark-1.4.0-bin-hadoop2.6
<span class="gp">$ </span>./bin/run-example SparkPi 10
</code></pre></div>
<p>This should give you some output like:</p>
<div class="highlight"><pre><code class="language-" data-lang="">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/08/21 10:52:56 INFO SparkContext: Running Spark version 1.4.0
...
15/08/21 10:53:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 736 bytes result sent to driver
15/08/21 10:53:02 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1446 bytes)
15/08/21 10:53:02 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/21 10:53:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 953 ms on localhost (1/10)
15/08/21 10:53:03 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 736 bytes result sent to driver
...
15/08/21 10:53:03 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/08/21 10:53:03 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 736 bytes result sent to driver
15/08/21 10:53:03 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 16 ms on localhost (10/10)
15/08/21 10:53:03 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:35) finished in 1.457 s
15/08/21 10:53:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
15/08/21 10:53:03 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:35, took 1.991463 s
</code></pre></div>
<p>Assuming you got the output like the above your Spark configuration is working and we are ready to get going with Spark.</p>

<p>Before we continue to configure PySpark, Python and IPython we will add some environment variables to our system that tell it where Spark is installed and what its default configuration is.</p>

<p>Open your bash-profile using <code>nano ~/.bash_profile</code> and add the following two lines:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Set the Spark Home as an environment variable.</span>
<span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span><span class="s2">"</span><span class="nv">$HOME</span><span class="s2">/spark-1.4.0-bin-hadoop2.6"</span>

<span class="c"># Define your Spark arguments for when running Spark.</span>
<span class="nb">export </span><span class="nv">PYSPARK_SUBMIT_ARGS</span><span class="o">=</span><span class="s2">"--master local[2]"</span>
</code></pre></div>
<h3>PySpark - Shell</h3>

<p>Now that we have Spark working we can use Python to actually give it our instructions. Spark comes with a <code>pySpark</code> shell that we can use for that. Launch the shell using:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./bin/pyspark
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">Python 2.7.6 (default, Mar 22 2014, 22:59:56)
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.4.0
      /_/

Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)
SparkContext available as sc, HiveContext available as sqlContext.
&gt;&gt;&gt; 
</code></pre></div>
<p>This brings you into the pySpark. You can now use python to use Spark. It has already created a <code>SparkContext</code> to work with/from.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">&gt;&gt;&gt; </span>sc
&lt;pyspark.context.SparkContext object at 0x7f05502a1750&gt;
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">dataRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">&gt;&gt;&gt; </span>dataRDD
ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:396
</code></pre></div>
<h3>PySpark - IPython Configuration</h3>

<p>First install <code>virtualenv</code> to allow us to work in a virtual environment.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Install pip, the python package manager.</span>
<span class="gp">$ </span>sudo apt-get install python-pip

<span class="c"># Install virtualenv</span>
<span class="gp">$ </span>sudo pip install virtualenv

<span class="c"># Create a virtual environment.</span>
<span class="gp">$ </span>virtualenv pyEnv
</code></pre></div>
<p>Now we will activate this environment so that we can install our Python libraries inside the virtual environment.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">source </span>pyEnv/bin/activate
</code></pre></div>
<p>Now we can install <code>IPython</code> within the virtual environment <code>pyEnv</code>. To install use the following command.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>sudo pip install <span class="s2">"ipython[notebook]"</span>
</code></pre></div>
<p>We now have IPython installed within our virtual environment. The next thing is to configure IPython such that it runs with a pySpark kernel and we can actually start using Spark from within IPython. We are going to attempt doing this by creating an IPython profile specifically for Spark.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Create a new IPython profile.</span>
<span class="gp">$ </span>ipython profile create pyspark
</code></pre></div>
<p>Now that we have created the actual <code>pyspark</code> profile for IPython we will need to configure it. Most of the configurations can be done in the file <code>ipython_notebook_config.py</code>. Open this file (I am using <code>nano</code> for the editing):</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>nano ~/.ipython/profile_pyspark/ipython_notebook_config.py
</code></pre></div>
<p>It will have a long list of comments and configuration lines that are most likely commented out. We are going to change 3 specific configuration elements. The first we will change is the broadcasting ip-address. We will tell it to broadcast on any IP-address (mind: this is not secure for production environments!). To do this add the following line:</p>
<div class="highlight"><pre><code class="language-" data-lang=""># c.NotebookApp.ip = 'localhost'
c.NotebookApp.ip = '*'
</code></pre></div>
<p>Since we are working on a (virtual) server we don&#39;t want IPython to open a browser as its default setting. To disable this add the following line:</p>
<div class="highlight"><pre><code class="language-" data-lang=""># c.NotebookApp.open_browser = True
c.NotebookApp.open_browser = False
</code></pre></div>
<p>IPython has a default port it is always opening up for connection. We have chosen to use a different port. You can change this by adding the following line:</p>
<div class="highlight"><pre><code class="language-" data-lang=""># c.NotebookApp.port = 8888
c.NotebookApp.port = 8001
</code></pre></div>
<p>Please mind, the above setting means that you have to have added port-forwarding to your <code>Vagrantfile</code> (we did in the beginning of this post). Your local system will actually need a port to forward to the IPython port to be able to see the notebooks.</p>

<p>The configuration of IPython is now done, but we need to make sure it actually creates a <code>SparkContext</code> on startup. We can do this by changing the IPython profile&#39;s setup file: <code>00-pyspark-setup.py</code>. Open this file in a text-editor:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>nano ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py
</code></pre></div>
<p>and paste the following Python script and save the contents afterwards.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Configure the necessary Spark environment</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c"># Set the spark_home variable</span>
<span class="n">SPARKHOME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SPARK_HOME'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">SPARKHOME</span> <span class="o">+</span> <span class="s">"/python"</span><span class="p">)</span>

<span class="c"># Add the py4j to the path.</span>
<span class="c"># You may need to change the version number to match your install</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">SPARKHOME</span><span class="p">,</span> <span class="s">'python/lib/py4j-0.8.2.1-src.zip'</span><span class="p">))</span>

<span class="c"># Initialize PySpark to predefine the SparkContext variable 'sc'</span>
<span class="nb">execfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">SPARKHOME</span><span class="p">,</span> <span class="s">'python/pyspark/shell.py'</span><span class="p">))</span>
</code></pre></div>
<p>To start IPython and have it use Spark we need to use quite a large command, so we&#39;ll create an alias for this in our <code>.bash_profile</code>. Open your bash-profile using <code>nano ~/.bash_profile</code> and add the following two lines:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># IPython alias for the use with SPARK.</span>
<span class="nb">alias </span><span class="nv">IPYSPARK</span><span class="o">=</span><span class="s1">'PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook --profile=pyspark --ip=0.0.0.0" $SPARK_HOME/bin/pyspark'</span>
</code></pre></div>
<p>After saving and closing make sure to reload the profile using <code>source ~/.bash_profile</code> so that all changes have taken effect. Now we can start IPython (using the Spark shell) using the just created alias:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">cd</span> <span class="nv">$HOME</span>
<span class="gp">$ </span>IPYSPARK
</code></pre></div>
<p>Now, on your local machine, open your webbrowser and navigate to <code>localhost:8001</code> and it should show the IPython notebook server. You are now good to go!</p>

<p>Some useful links:</p>

<ul>
<li>https://spark.apache.org/documentation.html</li>
<li>https://spark.apache.org/docs/latest/programming-guide.html</li>
<li>https://spark.apache.org/docs/latest/api/python/index.html</li>
<li>http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/</li>
<li>http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/</li>
<li>http://ipython.org/ipython-doc/1/interactive/public_server.html</li>
<li>https://docs.prediction.io/datacollection/analytics-ipynb/</li>
<li>Scala &amp; IPython (not working yet): https://github.com/alexarchambault/jupyter-scala</li>
<li>Scala &amp; IPython (not working yet): https://github.com/hohonuuli/sparknotebook</li>
</ul>

                </article>

                <!-- mathjax -->
                
                
            </div>
        </div>
            
            </main>

        </div>

        <footer class="site-footer">

    <!-- Google Analytics -->
    <script src="/js/ga.js"></script>
    <script src="/js/hotjar.js"></script>

</footer>


    </body>

</html>