<!DOCTYPE html>
<html>

    <head>

                
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        
        <title>Setting up an Apache Spark Cluster</title>
        
        <meta name="viewport" content="width=device-width">
        <meta name="description" content="Some thoughts and writeups.">
        
        <link rel="stylesheet" href="//storage.googleapis.com/code.getmdl.io/1.0.4/material.indigo-pink.min.css">
        <script src="//storage.googleapis.com/code.getmdl.io/1.0.4/material.min.js"></script>
        <!-- Icon-font -->
        <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
        <!-- Fonts -->
        <link href='//fonts.googleapis.com/css?family=Merriweather:400,300,700' rel='stylesheet' type='text/css'>
        <link href='//fonts.googleapis.com/css?family=Hind:300,400,500,600' rel='stylesheet' type='text/css'>

        <link rel="canonical" href="http://luckylwk.github.io/2015/08/21/setting-up-apache-spark-cluster/">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

        <link rel="icon" type="image/png" href="/assets/github-favicon.png">

        
        <style>
            .lwk-main-container {
                background: #f9f9f9;
            }
            .lwk-main-container .mdl-layout__header,
            .lwk-main-container .mdl-layout__drawer-button {
                color: white;
            }
            .mdl-layout__header {
                background: rgba(12,16,61,1);
            }
        </style>

    </head>

    <body>

        <div class="lwk-main-container mdl-layout mdl-js-layout">

            <!-- <div id="particles-js" class="particles-container"></div> -->
            <!-- // <script src="../js/particles.min.js"></script> -->
            <!-- // <script src="../js/app.js"></script> -->

                                

            <header class="mdl-layout__header mdl-layout__header">
                <div class="mdl-layout__header-row">
                    <!-- Title -->
                    <span class="mdl-layout-title">luckylwk</span>
                    <!-- Add spacer, to align navigation to the right -->
                    <div class="mdl-layout-spacer"></div>
                    <!-- Navigation -->
                    <nav class="mdl-navigation">
                        <a class="mdl-navigation__link" href="/machine-learning/">Machine Learning</a>
                        <a class="mdl-navigation__link" href="/data-visualisation/">Data Visualisation</a>
                        <a class="mdl-navigation__link" href="/data-engineering/">Data Engineering</a>
                        <a class="mdl-navigation__link" href="/about/">About</a>
                    </nav>
                </div>
            </header>
            <div class="mdl-layout__drawer">
                <span class="mdl-layout-title">luckylwk</span>
                <nav class="mdl-navigation">
                    <a class="mdl-navigation__link" href="/machine-learning/">Machine Learning</a>
                    <a class="mdl-navigation__link" href="/data-visualisation/">Data Visualisation</a>
                    <a class="mdl-navigation__link" href="/data-engineering/">Data Engineering</a>
                    <a class="mdl-navigation__link" href="/about/">About</a>
                    <a class="mdl-navigation__link" href="https://www.twitter.com/luckylwk" target="_blank">Twitter</a>
                    <a class="mdl-navigation__link" href="https://uk.linkedin.com/pub/luuk-derksen/a/80/956" target="_blank">LinkedIn</a>
                    <a class="mdl-navigation__link" href="https://github.com/luckylwk" target="_blank">Github</a>
                </nav>
            </div>

            <main class="mdl-layout__content">

                        <div class="wrap">
            <div class="post">

                <header class="post-header">
                    <h1>Setting up an Apache Spark Cluster</h1>
                    <p class="meta">Aug 21, 2015</p>
                    <a href="https://twitter.com/share" class="twitter-share-button" data-text="Check out Setting up an Apache Spark Cluster" data-via="luckylwk">Tweet</a>
                    <script>
                        !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');
                    </script>
                </header>

                <article class="post-content">
                <h3>Installing Spark/pySpark</h3>

<p>We will not deal with the actual setup of Spark and IPython on each machine in this post. To get that done you can follow the <a href="http://luckylwk.github.io/2015/07/30/setting-up-apache-spark/">instructions in this blog-post</a></p>

<h3>Configuring the Spark Cluster Manager</h3>

<p>The first thing we will need is for the <strong>manager</strong> machine to be able to SSH into the <strong>worker</strong> machines. To do this we need to create an SSH-keypair.</p>

<h5>1. Create SSH Keypair</h5>

<p>Create an <a href="https://golog.co/blog/article/SSH_-_Creating_and_using_SSH-keys">SSH-keypair</a> using the following commands:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Move into the .ssh folder</span>
<span class="gp">$ </span><span class="nb">cd</span> ~/.ssh

<span class="c"># Create a keypair.</span>
<span class="gp">$ </span>ssh-keygen -t rsa
</code></pre></div>
<p>When asked for a name, name it something appropriate, like: <code>sparkManagerKey</code></p>

<h5>2. Worker Shortcuts</h5>

<p>Open the SSH config file using <code>nano ~/.ssh/config</code> and paste the following configuration settings for the penguins:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">Host worker.one
        HostName XXX.XXX.XXX.XXX
        Port 22
        User &lt;USERNAME HERE&gt;
        IdentityFile ~/.ssh/sparkManagerKey

Host worker.two
        HostName XXX.XXX.XXX.XXX
        Port 22
        User &lt;USERNAME HERE&gt;
        IdentityFile ~/.ssh/sparkManagerKey   
</code></pre></div>
<p>The benefit of doing this is that you can now refer to the worker machines using their given names. So if you want to SSH into one of the machines from your manager machine all you need to do is <code>ssh worker.one</code>. However, this will not work now as the worker machine does not accept any incoming SSH-connections from this machine yet. We&#39;ll deal with this in a second, first we&#39;ll finish setting up the manager machine.</p>

<h5>3. Configure the Spark Manager</h5>

<p>Now we will need to configure the <strong>manager</strong> machine to know about its <strong>workers</strong>. Navigate into the spark installation folder and then go to the conf-directory (<code>cd $SPARK_HOME/conf</code>). There are a number of files present here with the extension <code>.template</code>. To look at the contents of the directory:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>ls -la

drwxr-xr-x  2 spark.manager spark.manager 4096 Jun 19 17:06 .
drwxr-xr-x 13 spark.manager spark.manager 4096 Jun 19 17:06 ..
-rw-r--r--  1 spark.manager spark.manager  202 Jun  3 02:30 docker.properties.template
-rw-r--r--  1 spark.manager spark.manager  303 Jun  3 02:30 fairscheduler.xml.template
-rw-r--r--  1 spark.manager spark.manager  632 Jun  3 02:30 log4j.properties.template
-rw-r--r--  1 spark.manager spark.manager 5565 Jun  3 02:30 metrics.properties.template
-rw-r--r--  1 spark.manager spark.manager   80 Jun  3 02:30 slaves.template
-rw-r--r--  1 spark.manager spark.manager  507 Jun  3 02:30 spark-defaults.conf.template
-rwxr-xr-x  1 spark.manager spark.manager 3318 Jun  3 02:30 spark-env.sh.template
</code></pre></div>
<p>We&#39;ll copy the one named <code>slaves.template</code> and configure the workers there.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>cp slaves.template slaves

<span class="gp">$ </span>nano slaves
</code></pre></div>
<p>When opened paste the following configuration.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Slaves file.</span>
worker.one
worker.two
</code></pre></div>
<p>Here you can see how nice it is to have &#39;shortcut&#39; names for the workers. If we every want to change something about their configuration we only need to change the SSH config file and Spark will know instantly.</p>

<p>Next copy the <strong>manager</strong> <code>spark-env.sh</code> file and open it using </p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>cp spark-env.sh.template spark-env.sh

<span class="gp">$ </span>nano spark-env.sh
</code></pre></div>
<p>In this file there are a lot of configuration parameters that can be set. We only need a few for this specific occasion. Set the following parameters and make sure to replace <code>&lt;MANAGER_IP_ADDRESS&gt;</code> with the IP-address of your Spark <strong>manager</strong> machine:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">SPARK_LOCAL_IP</span><span class="o">=</span>&lt;MANAGER_IP_ADDRESS&gt;
<span class="nv">SPARK_MASTER_IP</span><span class="o">=</span>&lt;MANAGER_IP_ADDRESS&gt;
<span class="nv">SPARK_WORKER_CORES</span><span class="o">=</span>1
<span class="nv">SPARK_WORKER_MEMORY</span><span class="o">=</span>1000m
</code></pre></div>
<p>Note: The IP-address used here is the IP-address of the <strong>manager</strong> (or master) machine.</p>

<p>The last thing to do on the <strong>manager</strong> machine is changing the IPython startup command slightly. When starting IPython/Spark as the <strong>master/manager</strong>, the command is slightly different as we have to tell it that we are running it as a manager and not a standalone instance. Open up the <code>.bash_profile</code>:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>nano ~/.bash_profile
</code></pre></div>
<p>and add the following line to it:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">alias </span><span class="nv">IPYSPARKMASTER</span><span class="o">=</span><span class="s1">'MASTER=spark://&lt;MANAGER_IP_ADDRESS&gt;:7077 PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook --profile=pyspark --ip=0.0.0.0" $SPARK_HOME/bin/pyspark'</span>
</code></pre></div>
<p>Note: here again we are using the IP address of the <strong>master/manager</strong> where it states <code>&lt;MANAGER_IP_ADDRESS&gt;</code>.</p>

<p>Therefore rather than typing <code>IPYSPARK</code> (because this is the shortcut in the bash profile), we can later on start IPython in Spark-cluster mode using <code>IPYSPARKMASTER</code>.</p>

<p>These were all the configurations that need to be setup for the <strong>master/manager</strong>. We now need to configure the <strong>workers</strong> so they know what their role is and who is managing them.</p>

<h3>Configuring Spark Workers</h3>

<p>The following instructions will deal with setting up the <strong>worker</strong> machines. Note that its best to complete the above instructions before setting up the workers as you will need bits and pieces from the above here.</p>

<h5>1. Add the Manager&#39;s SSH-Keypair</h5>

<p>For the <strong>manager</strong> machine to be able to SSH into the <strong>worker</strong> machine the worker needs to first have its SSH key listed as an authorized key.</p>

<p>On the <strong>manager</strong> machine print the contents of the public key using</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>cat ~/.ssh/sparkManagerKey.pub
</code></pre></div>
<p>Copy the contents and, on the <strong>worker</strong> machine open the <code>authorized_keys</code> file in a text-editor</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>nano ~/.ssh/authorized_keys
</code></pre></div>
<p>Paste in the contents of the public-key you just copied on a new line and save-and-close the file.</p>

<p>Now your <strong>worker</strong> machine will accept SSH connections from the <strong>manager</strong> machine. You can try this by trying to connect from the manager machine using <code>ssh worker.one</code>.</p>

<h5>2. Create an alias for the Manager Machine</h5>

<p>Add the IP address of the <strong>manager</strong> machine to the <code>.bash_profile</code> file</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>nano ~/.bash_profile
</code></pre></div>
<p>and paste in the following</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># paste in the following:</span>
<span class="nb">alias </span><span class="nv">SPARK_MANAGER_IP</span><span class="o">=</span><span class="s1">'&lt;MANAGER_IP_ADDRESS&gt;'</span>
</code></pre></div>
<p>Note: here again we are using the IP address of the <strong>master/manager</strong> where it states <code>&lt;MANAGER_IP_ADDRESS&gt;</code>.</p>

<h5>3. Configuring the Spark Worker</h5>

<p>On the <strong>worker</strong> machine navigate into the Spark configuration folder (<code>cd $SPARK_HOME/conf</code>).</p>

<p>Just like with the <strong>manager</strong> machine we need to configure the Spark environment for this <strong>worker</strong> machine. Copy the <code>spark-env.sh</code> and open it using a text-editor</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>cp spark-env.sh.template spark-env.sh

<span class="gp">$ </span>nano spark-env.sh
</code></pre></div>
<p>Now we need to add some configurations to this file so the <strong>worker</strong> machine knows about who to report to. Add the following lines</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">SPARK_MASTER_IP</span><span class="o">=</span>SPARK_MANAGER_IP 
<span class="nv">SPARK_WORKER_CORES</span><span class="o">=</span>1
<span class="nv">SPARK_WORKER_MEMORY</span><span class="o">=</span>1000m
</code></pre></div>
<p>Note: the <code>SPARK_MANAGER_IP</code> alias was added before in the <code>~/.bash_profile</code>.</p>

<h5>4. Spark Worker Path</h5>

<p>Since we are working from a <strong>Vagrant</strong> virtual machine this is where we run into a complication. The workers have a Spark installation similar to <code>/home/&lt;MACHINE_NAME&gt;/spark_installation</code>. However the <strong>manager</strong> machine is SSH-ing in as vagrant (or some other username) and is looking for <code>home/vagrant/spark_installation</code>. To work around this we will create a symbolic link inside the <strong>worker</strong> machine that reflects the <strong>manager</strong> machine&#39;s login and points to the actual installation path. So, in the workers create symbolic links for whatever the manager will do. </p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>sudo mkdir /home/&lt;MANAGER_NAME&gt;

<span class="gp">$ </span>sudo ln -s /home/&lt;WORKER_NAME&gt;/spark-1.4.0-bin-hadoop2.6/ /home/&lt;MANAGER_NAME&gt;/spark-1.4.0-bin-hadoop2.6
</code></pre></div>
<p>Note: in a production environment, make sure the same user is configured in all machines (this is the problem here)?</p>

<h3>Start the Cluster and use IPython</h3>

<p>To run the cluster we need to go to the <strong>manager</strong> machine and run the following command from within the Spark folder <code>$SPARK_HOME</code>.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Start the master and worker...</span>
<span class="gp">$ </span>.sbin/start-all.sh
</code></pre></div>
<p>Now to start IPython we will want to nagivate to the folder we will want to start it from.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Navigate to the HOME folder...</span>
<span class="gp">$ </span><span class="nb">cd</span> <span class="nv">$HOME</span>

<span class="c"># Start IPython.</span>
<span class="gp">$ </span>IPYSPARKMASTER
</code></pre></div>
<ol>
<li>go to <code>http://&lt;master_ip_address or 127.0.0.1&gt;:8001/tree</code> for pynotebooks</li>
<li>go to <code>http://&lt;master_ip_address or 127.0.0.1&gt;:8080</code> for overview GUI (starts working after start all)</li>
<li>go to <code>http://&lt;master_ip_address or 127.0.0.1&gt;:4040</code> to see jobs (starts working after the python kernel has started up)</li>
</ol>

<p>When finished, quit the notebook and don&#39;t forget to stop all the services using: </p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">cd</span> <span class="nv">$SPARK_HOME</span>

<span class="gp">$ </span>.sbin/stop-all.sh
</code></pre></div>
                </article>

                <!-- mathjax -->
                
                <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                
                
            </div>
        </div>
            
            </main>

        </div>

        <footer class="site-footer">

    <!-- Google Analytics -->
    <script src="/js/ga.js"></script>
    <script src="/js/hotjar.js"></script>

</footer>


    </body>

</html>